@preconcurrency import EventSource
import Foundation
import SwiftUI
import os.lock

func removeUnmatchedTrailingQuote(_ inputString: String) -> String {
	var outputString = inputString
	if inputString.last != "\"" { return outputString }
	
	// Count the number of quotes in the string
	let countOfQuotes = outputString.reduce(
		0,
		{ (count, character) -> Int in
			return character == "\"" ? count + 1 : count
		})
	
	// If there is an odd number of quotes, remove the last one
	if countOfQuotes % 2 != 0 {
		if let indexOfLastQuote = outputString.lastIndex(of: "\"") {
			outputString.remove(at: indexOfLastQuote)
		}
	}
	
	return outputString
}

actor LlamaServer {
	
	var modelPath: String?
	var modelName: String {
		modelPath?.split(separator: "/").last?.map { String($0) }.joined() ?? "unknown"
	}
	
	var contextLength: Int
	
	@AppStorage("useGPU") private var useGPU: Bool = DEFAULT_USE_GPU
	
	private let gpu = GPU.shared
	private var process = Process()
	private var serverUp = false
	private var serverErrorMessage = ""
	private var eventSource: EventSource?
	private let host: String
	private let port: String
	private let scheme: String
	private var interrupted = false
	
	private var monitor = Process()
	
	init(modelPath: String, contextLength: Int) {
		self.modelPath = modelPath
		self.contextLength = contextLength
		self.scheme = "http"
		self.host = "127.0.0.1"
		self.port = "8690"
	}
	
	init(contextLength: Int, tls: Bool, host: String, port: String) {
		self.contextLength = contextLength
		self.scheme = tls ? "https" : "http"
		self.host = host
		self.port = port
	}
	
	private func url(_ path: String) -> URL {
		URL(string: "\(scheme)://\(host):\(port)\(path)")!
	}
	
	// Start a monitor process that will terminate the server when our app dies.
	private func startAppMonitor(serverPID: pid_t) throws {
		monitor = Process()
		monitor.executableURL = Bundle.main.url(forAuxiliaryExecutable: "server-watchdog")
		monitor.arguments = [String(serverPID)]
		
#if DEBUG
		print(
			"Starting \(monitor.executableURL!.absoluteString) \(monitor.arguments!.joined(separator: " "))"
		)
#endif
		
		let hearbeat = Pipe()
		// write a heartbeat to the pipe every 10 seconds
		let timer = DispatchSource.makeTimerSource(queue: DispatchQueue.global())
		timer.schedule(deadline: .now(), repeating: 10.0)
		timer.setEventHandler { [weak hearbeat] in
			guard let hearbeat = hearbeat else { return }
			let data = ".".data(using: .utf8) ?? Data()
			hearbeat.fileHandleForWriting.write(data)
		}
		timer.resume()
		
		monitor.standardInput = hearbeat
		
#if !DEBUG
		let monitorOutputPipe = Pipe()
		monitor.standardOutput = monitorOutputPipe
		monitor.standardError = monitorOutputPipe
#endif
		
		try monitor.run()
		
		print("Started monitor for \(serverPID)")
	}
	
	private func startServer() async throws {
		guard !process.isRunning, let modelPath = self.modelPath else { return }
		stopServer()
		process = Process()
		
		let startTime = DispatchTime.now()
		
		process.executableURL = Bundle.main.url(forAuxiliaryExecutable: "freechat-server")
		let processes = ProcessInfo.processInfo.activeProcessorCount
			
		let gpuLayers: Int = max(min(GPU.coreCount + 5, 35), 30)
		
		process.arguments = [
			"--model", modelPath,
			"--threads", "\(max(1, Int(ceil(Double(processes) / 3.0 * 2.0))))",
			"--ctx-size", "\(contextLength)",
			"--port", port,
			"--n-gpu-layers", gpu.available && useGPU ? "\(gpuLayers)" : "0",
		]
		
		print("Starting llama.cpp server \(process.arguments!.joined(separator: " "))")
		
		process.standardInput = FileHandle.nullDevice
		
		// To debug with server's output, comment these 2 lines to inherit stdout.
		process.standardOutput =  FileHandle.nullDevice
		process.standardError =  FileHandle.nullDevice
		
		try process.run()
		
		try await waitForServer()
		
		try startAppMonitor(serverPID: process.processIdentifier)
		
		let endTime = DispatchTime.now()
		let elapsedTime = Double(endTime.uptimeNanoseconds - startTime.uptimeNanoseconds) / 1_000_000
		
#if DEBUG
		print("Started server process in \(elapsedTime)ms")
#endif
	}
	
	func stopServer() {
		if process.isRunning {
			process.terminate()
		}
		if monitor.isRunning {
			monitor.terminate()
		}
	}
	
	func chat(
		messages: [LlamaServer.ChatMessage],
		temperature: Double?,
		progressHandler: (@Sendable (String) -> Void)? = nil
	) async throws -> CompleteResponse {
		
		let start = CFAbsoluteTimeGetCurrent()
		try await startServer()
		
		// hit localhost for completion
		var params = ChatParams(
			messages: messages
		)
		if let t = temperature { params.temperature = t }
		
		var request = URLRequest(url: url("/v1/chat/completions"))
		
		request.httpMethod = "POST"
		request.setValue("application/json", forHTTPHeaderField: "Content-Type")
		request.setValue("text/event-stream", forHTTPHeaderField: "Accept")
		request.setValue("keep-alive", forHTTPHeaderField: "Connection")
		request.httpBody = params.toJSON().data(using: .utf8)
		
		// Use EventSource to receive server sent events
		eventSource = EventSource(request: request)
		eventSource!.connect()
		
		var response = ""
		var responseDiff = 0.0
		var stopResponse: StopResponse?
	listenLoop: for await event in eventSource!.events {
		switch event {
			case .open:
				continue listenLoop
			case .error(let error):
				print("llama.cpp EventSource server error:", error.localizedDescription)
			case .message(let message):
				// parse json in message.data string then print the data.content value and append it to response
				if let data = message.data?.data(using: .utf8) {
					let decoder = JSONDecoder()
					
					do {
						let responseObj = try decoder.decode(StreamResponse.self, from: data)
						let fragment = responseObj.choices[0].delta.content ?? ""
						response.append(fragment)
						progressHandler?(fragment)
						if responseDiff == 0 {
							responseDiff = CFAbsoluteTimeGetCurrent() - start
						}
						
						if responseObj.choices[0].finish_reason != nil {
							do {
								stopResponse = try decoder.decode(StopResponse.self, from: data)
							} catch {
								print("Error decoding stopResponse", error as Any, data)
							}
#if DEBUG
							print(
								"server.cpp stopResponse",
								NSString(data: data, encoding: String.Encoding.utf8.rawValue) ?? "missing")
#endif
							break listenLoop
						}
					} catch {
						print("Error decoding responseObj", error as Any, String(decoding: data, as: UTF8.self))
						break listenLoop
					}
				}
			case .closed:
				print("llama.cpp EventSource closed")
				break listenLoop
		}
	}
		
		if responseDiff > 0 {
			print("Response: \(response)")
			print("\n\nBot started response in \(responseDiff) seconds")
		}
		
		// adding a trailing quote or space is a common mistake with the smaller model output
		let cleanText = removeUnmatchedTrailingQuote(response).trimmingCharacters(
			in: .whitespacesAndNewlines)
		
		let tokens = stopResponse?.usage.completion_tokens ?? 0
		return CompleteResponse(
			text: cleanText,
			responseStartSeconds: responseDiff,
			predictedPerSecond: Double(tokens) / responseDiff,
			modelName: modelName,
			nPredicted: tokens
		)
	}
	
	func interrupt() async {
		if let eventSource, eventSource.readyState != .closed {
			await eventSource.close()
		}
		interrupted = true
	}
	
	private func waitForServer() async throws {
		guard process.isRunning else { return }
		interrupted = false
		serverErrorMessage = ""
		
		let serverHealth = ServerHealth()
		await serverHealth.updateURL(url("/health"))
		await serverHealth.check()
		
		var timeout = 60
		let tick = 1
		while true {
			await serverHealth.check()
			let score = await serverHealth.score
			if score >= 0.25 { break }
			await serverHealth.check()
			if !process.isRunning {
				throw LlamaServerError.modelError(modelName: modelName)
			}
			
			try await Task.sleep(for: .seconds(tick))
			timeout -= tick
			if timeout <= 0 {
				throw LlamaServerError.modelError(modelName: modelName)
			}
		}
	}
	
	struct CompleteResponse {
		var text: String
		var responseStartSeconds: Double
		var predictedPerSecond: Double?
		var modelName: String?
		var nPredicted: Int?
	}
	
	enum ChatRole: Codable {
		case system
		case user
	}
	
	struct ChatMessage: Codable {
		var role: ChatRole
		var content: String
		
		func toJSON() -> String {
			let encoder = JSONEncoder()
			encoder.outputFormatting = .prettyPrinted
			let jsonData = try? encoder.encode(self)
			return String(data: jsonData!, encoding: .utf8)!
		}
	}
	
	struct ChatParams: Codable {
		var messages: [ChatMessage]
		var stream = true
		var n_threads = 6
		
		var n_predict = -1
		var temperature = DEFAULT_TEMP
		var repeat_last_n = 128  // 0 = disable penalty, -1 = context size
		var repeat_penalty = 1.18  // 1.0 = disabled
		var top_k = 40  // <= 0 to use vocab size
		var top_p = 0.95  // 1.0 = disabled
		var tfs_z = 1.0  // 1.0 = disabled
		var typical_p = 1.0  // 1.0 = disabled
		var presence_penalty = 0.0  // 0.0 = disabled
		var frequency_penalty = 0.0  // 0.0 = disabled
		var mirostat = 0  // 0/1/2
		var mirostat_tau = 5  // target entropy
		var mirostat_eta = 0.1  // learning rate
		var cache_prompt = true
		
		func toJSON() -> String {
			let encoder = JSONEncoder()
			encoder.outputFormatting = .prettyPrinted
			let jsonData = try? encoder.encode(self)
			return String(data: jsonData!, encoding: .utf8)!
		}
	}
	
	struct StreamMessage: Codable {
		let content: String?
	}
	
	struct StreamChoice: Codable {
		let delta: StreamMessage
		let finish_reason: String?
	}
	
	struct StreamResponse: Codable {
		let choices: [StreamChoice]
	}
	
	struct Usage: Codable {
		let completion_tokens: Int?
		let prompt_tokens: Int?
		let total_tokens: Int?
	}
	
	struct StopResponse: Codable {
		let choices: [StreamChoice]
		let usage: Usage
	}
}

enum LlamaServerError: LocalizedError {
	var errorDescription: String? {
		switch self {
			case .modelError(let modelName):
				return "Error Loading (\(modelName))"
			default:
				return "Llama Server Error"
		}
	}
	
	var recoverySuggestion: String {
		switch self {
			case .modelError:
				return "Try selecting another model in Settings"
			default:
				return "Try again later"
		}
	}
	
	case pipeFail
	case jsonEncodingError
	case modelError(modelName: String)
}
